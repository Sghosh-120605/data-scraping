import pandas as pd
import time
import os
from datetime import datetime
from dateutil import parser

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager


CSV_FILE = "hackathons_detailed.csv"


# =========================
# DRIVER
# =========================
def get_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--start-maximized")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=options
    )
    return driver


# =========================
# SCROLL FUNCTION
# =========================
def scroll_page(driver):
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height


# =========================
# GENERIC PAGE SCRAPER
# =========================
def scrape_full_page(driver, url, platform):
    driver.get(url)
    time.sleep(4)

    data = {}
    data["Platform"] = platform
    data["Link"] = url

    # Hackathon Name
    try:
        data["Name"] = driver.find_element(By.TAG_NAME, "h1").text
    except:
        data["Name"] = None

    # Extract headings dynamically
    headings = driver.find_elements(By.XPATH, "//h2 | //h3")

    for h in headings:
        try:
            title = h.text.strip()
            parent = h.find_element(By.XPATH, "./..")
            content = parent.text.replace(title, "").strip()

            if title and content:
                data[title] = content
        except:
            continue

    # Extract timeline-related keywords
    page_text = driver.page_source
    keywords = ["Start", "End", "Deadline", "Round", "Registration"]

    for line in page_text.split("\n"):
        for keyword in keywords:
            if keyword.lower() in line.lower():
                data[f"{keyword}_Info"] = line.strip()

    return data


# =========================
# DEVFOLIO LINKS (FIXED)
# =========================
def get_devfolio_links(driver):
    driver.get("https://devfolio.co/hackathons")
    time.sleep(6)

    scroll_page(driver)

    links = []
    elements = driver.find_elements(By.TAG_NAME, "a")

    for el in elements:
        link = el.get_attribute("href")

        if not link:
            continue

        if "devfolio.co" not in link:
            continue

        skip_words = [
            "login",
            "blog",
            "terms",
            "privacy",
            "about",
            "organize",
            "dashboard"
        ]

        if any(word in link.lower() for word in skip_words):
            continue

        if link.rstrip("/") in [
            "https://devfolio.co",
            "https://devfolio.co/hackathons"
        ]:
            continue

        links.append(link)

    clean_links = list(set(links))

    print("Raw Devfolio links found:", len(clean_links))

    return clean_links


# =========================
# UNSTOP LINKS (UNCHANGED)
# =========================
def get_unstop_links(driver):
    driver.get("https://unstop.com/hackathons")
    time.sleep(5)
    scroll_page(driver)

    links = []
    elements = driver.find_elements(By.XPATH, "//a[contains(@href,'/hackathons/')]")

    for el in elements:
        link = el.get_attribute("href")
        if link:
            links.append(link)

    return list(set(links))


# =========================
# REMOVE EXPIRED
# =========================
def remove_expired(df):
    today = datetime.now()

    date_cols = [col for col in df.columns if "End" in col or "Deadline" in col]

    for col in date_cols:
        df[col] = pd.to_datetime(df[col], errors="coerce")

    for col in date_cols:
        df = df[(df[col].isna()) | (df[col] >= today)]

    return df


# =========================
# MAIN
# =========================
def main():

    driver = get_driver()

    print("Collecting Devfolio links...")
    devfolio_links = get_devfolio_links(driver)

    print("Collecting Unstop links...")
    unstop_links = get_unstop_links(driver)

    print("Devfolio Found:", len(devfolio_links))
    print("Unstop Found:", len(unstop_links))

    all_data = []

    # LIMIT FOR TESTING (remove [:15] after confirmed working)
    for i, link in enumerate(devfolio_links[:15]):
        print(f"Devfolio {i+1}")
        try:
            all_data.append(scrape_full_page(driver, link, "Devfolio"))
        except:
            continue

    for i, link in enumerate(unstop_links[:15]):
        print(f"Unstop {i+1}")
        try:
            all_data.append(scrape_full_page(driver, link, "Unstop"))
        except:
            continue

    driver.quit()

    df = pd.DataFrame(all_data)

    df = remove_expired(df)

    if os.path.exists(CSV_FILE):
        old_df = pd.read_csv(CSV_FILE)
        df = pd.concat([old_df, df], ignore_index=True)

    df.drop_duplicates(subset=["Name", "Platform"], inplace=True)

    df.to_csv(CSV_FILE, index=False)

    print("âœ… Unified Detailed Dataset Created")
    print("Total Active Hackathons:", len(df))


if __name__ == "__main__":
    main()